
@article{de_maat_machine_2010,
	title = {Machine Learning versus Knowledge Based Classification of Legal Texts},
	url = {https://ebooks.iospress.nl/doi/10.3233/978-1-60750-682-9-87},
	doi = {10.3233/978-1-60750-682-9-87},
	pages = {87--96},
	journaltitle = {Legal Knowledge and Information Systems},
	author = {de Maat, Emile and Krabben, Kai and Winkels, Radboud},
	urldate = {2023-01-06},
	date = {2010},
	note = {Publisher: {IOS} Press},
}

@article{beach_rule-based_2015,
	title = {A rule-based semantic approach for automated regulatory compliance in the construction sector},
	volume = {42},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417415001360},
	doi = {10.1016/j.eswa.2015.02.029},
	abstract = {A key concern for professionals in any industry is ensuring regulatory compliance. Regulations are often complex and require in depth technical knowledge of the domain in which they operate. The level of technical detail and complexity in regulations is a barrier to their automation due to extensive software development time and costs that are involved. In this paper we present a rule-based semantic approach formulated as a methodology to overcome these issues by allowing domain experts to specify their own regulatory compliance systems without the need for extensive software development. Our methodology is based on the key idea that three semantic contexts are needed to fully understand the regulations being automated: the semantics of the target domain, the specific semantics of regulations being considered, and the semantics of the data format that is to be checked for compliance. This approach allows domain experts to create and maintain their own regulatory compliance systems, within a semantic domain that is familiar to them. At the same time, our approach allows for the often diverse nature of semantics within a particular domain by decoupling the specific semantics of regulations from the semantics of the domain itself. This paper demonstrates how our methodology has been validated using a series of regulations automated by professionals within the construction domain. The regulations that have been developed are then in turn validated on real building data stored in an industry specific format (the {IFCs}). The adoption of this methodology has greatly advanced the process of automating these complex sets of construction regulations, allowing the full automation of the regulation scheme within 18months. We believe that these positive results show that, by adopting our methodology, the barriers to the building of regulatory compliance systems will be greatly lowered and the adoption of three semantic domains proposed by our methodology provides tangible benefits.},
	pages = {5219--5231},
	number = {12},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Beach, T. H. and Rezgui, Y. and Li, H. and Kasim, T.},
	urldate = {2023-01-06},
	date = {2015-07-15},
	langid = {english},
	keywords = {Compliance checking, Construction industry, Regulations, Regulatory compliance, Rule engine, Semantics of regulations, Semantics of Regulatory Compliance},
	file = {Akzeptierte Version:/Users/jacobfehn/Zotero/storage/Q4HT78CE/Beach et al. - 2015 - A rule-based semantic approach for automated regul.pdf:application/pdf;ScienceDirect Snapshot:/Users/jacobfehn/Zotero/storage/QKULVR5Y/S0957417415001360.html:text/html},
}

@inproceedings{hashmi_methodology_2015,
	title = {A Methodology for Extracting Legal Norms from Regulatory Documents},
	doi = {10.1109/EDOCW.2015.29},
	abstract = {We present a methodology to extract legal norms from regulatory documents for their formalisation and later compliance checking. The need for the methodology is motivated from the shortcomings of existing approaches where the rule type and process aspects relevant to the rules are largely overlook. The methodology incorporates the well-known {IF}... {THEN} structure extended with the process aspect and rule type, and guides how to properly extract the conditions and logical structure of the legal rules for reasoning and modelling of obligations for compliance checking.},
	eventtitle = {2015 {IEEE} 19th International Enterprise Distributed Object Computing Workshop},
	pages = {41--50},
	booktitle = {2015 {IEEE} 19th International Enterprise Distributed Object Computing Workshop},
	author = {Hashmi, Mustafa},
	date = {2015-09},
	note = {{ISSN}: 2325-6605},
	keywords = {Business Processes Compliance, Context, Contracts, Data mining, Law, Legal documents, Norms, Norms extraction, Norms types, Process control},
	file = {IEEE Xplore Abstract Record:/Users/jacobfehn/Zotero/storage/FJGBQZFD/7310669.html:text/html},
}

@article{zhang_semantic_2016,
	title = {Semantic {NLP}-Based Information Extraction from Construction Regulatory Documents for Automated Compliance Checking},
	volume = {30},
	issn = {0887-3801, 1943-5487},
	url = {https://ascelibrary.org/doi/10.1061/%28ASCE%29CP.1943-5487.0000346},
	doi = {10.1061/(ASCE)CP.1943-5487.0000346},
	abstract = {Automated regulatory compliance checking requires automated extraction of requirements from regulatory textual documents and their formalization in a computer-processable rule representation. Such information extraction ({IE}) is a challenging task that requires complex analysis and processing of text. Natural Language Processing ({NLP}) aims at enabling computers to process natural language text in a human-like manner. This paper proposes a semantic, rule-based {NLP} approach for automated {IE} from construction regulatory documents. In our proposed approach, we use a set of pattern-matching-based {IE} rules and conflict resolution ({CR}) rules in {IE}. We use a variety of syntactic (syntax/grammar-related) and semantic (meaning/context-related) text features in the patterns of the {IE} and {CR} rules. We also propose and use phrase structure grammar ({PSG})-based phrasal tags and separation and sequencing of semantic information elements to reduce number of needed patterns. We utilize an ontology to aid in the recognition of semantic text features (concepts and relations). We tested our proposed {IE} extraction algorithms in extracting quantitative requirements from the 2009 International Building Code and achieved 0.969 and 0.944 precision and recall, respectively.},
	pages = {04015014},
	number = {2},
	journaltitle = {Journal of Computing in Civil Engineering},
	shortjournal = {J. Comput. Civ. Eng.},
	author = {Zhang, Jiansong and El-Gohary, Nora M.},
	urldate = {2023-01-06},
	date = {2016-03},
	langid = {english},
	file = {Zhang und El-Gohary - 2016 - Semantic NLP-Based Information Extraction from Con.pdf:/Users/jacobfehn/Zotero/storage/TRXZYLD2/Zhang und El-Gohary - 2016 - Semantic NLP-Based Information Extraction from Con.pdf:application/pdf},
}

@inproceedings{dinesh_extracting_2006,
	title = {Extracting formal specifications from natural language regulatory documents},
	url = {https://aclanthology.org/W06-3902},
	eventtitle = {{ICoS} 2006},
	booktitle = {Proceedings of the Fifth International Workshop on Inference in Computational Semantics ({ICoS}-5)},
	author = {Dinesh, Nikhil and Joshi, Aravind and Lee, Insup and Webber, Bonnie},
	urldate = {2023-01-06},
	date = {2006},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/X9M5MIFF/Dinesh et al. - 2006 - Extracting formal specifications from natural lang.pdf:application/pdf},
}

@inproceedings{winter_regminer_2020,
	title = {{RegMiner}: Taming the Complexity of Regulatory Documents for Digitalized Compliance Management},
	url = {https://www.semanticscholar.org/paper/RegMiner%3A-Taming-the-Complexity-of-Regulatory-for-Winter-Gall/1a973f646bc1d0548a32d14c4dfe1066c519076e},
	shorttitle = {{RegMiner}},
	abstract = {Business process compliance has become a crucial aspect for companies due to severe fines that can be imposed if constraints and rules emerging from regulatory documents are violated. Regulatory documents are often written in natural language and analyzing them is mainly done manually since only limited tool support is available. Therefore, we present {RegMiner}, a web service for discovering and visualizing constraints from regulatory documents. By employing {NLP} and data mining techniques, compliance constraints can be automatically extracted, grouped, and visualized leading to a separation of relevant and nonrelevant document parts and insights into, e.g., duties of stakeholders. A case study based on a current document from the European parliament regarding the financial domain demonstrates {RegMiner}’s maturity.},
	eventtitle = {International Conference on Business Process Management},
	author = {Winter, Karolin and Gall, Manuel and Rinderle-Ma, S.},
	urldate = {2023-01-06},
	date = {2020},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/D7HEAJ2A/Winter et al. - 2020 - RegMiner Taming the Complexity of Regulatory Docu.pdf:application/pdf},
}

@inproceedings{asooja_automatic_2017,
	title = {Automatic Detection of Significant Updates in Regulatory Documents},
	volume = {302},
	doi = {10.3233/978-1-61499-838-9-165},
	series = {Frontiers in Artificial Intelligence and Applications},
	pages = {165--169},
	booktitle = {Legal Knowledge and Information Systems - {JURIX} 2017: The Thirtieth Annual Conference, Luxembourg, 13-15 December 2017},
	publisher = {{IOS} Press},
	author = {Asooja, Kartik and Foghlú, Oscar Ó and Domhnaill, Breiffni Ó and Marchin, George and {McGrath}, Sean},
	editor = {Wyner, Adam Z. and Casini, Giovanni},
	date = {2017},
}

@article{glaser_classifying_2018,
	title = {Classifying Semantic Types of Legal Sentences: Portability of Machine Learning Models},
	url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-935-5-61},
	doi = {10.3233/978-1-61499-935-5-61},
	shorttitle = {Classifying Semantic Types of Legal Sentences},
	pages = {61--70},
	journaltitle = {Legal Knowledge and Information Systems},
	author = {Glaser, Ingo and Scepankova, Elena and Matthes, Florian},
	urldate = {2023-01-06},
	date = {2018},
	note = {Publisher: {IOS} Press},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/9RAQR8MM/Glaser et al. - 2018 - Classifying Semantic Types of Legal Sentences Por.pdf:application/pdf},
}

@inproceedings{waltl_classifying_2017,
	title = {Classifying Legal Norms with Active Machine Learning},
	abstract = {This paper describes an extended machine learning approach to classify legal norms in German statutory texts. We implemented an active machine learning ({AML}) framework based on open-source software. We discuss different query strategies to optimize the selection of instances during the learning phase to decrease the required training data. The approach was evaluated within the domain of tenancy law. We manually labeled the 532 sentences into eight different functional types and achieved an average F1 score of 0.74. Comparing three different clas-sifiers and four query strategies the classification performance F1 varies from 0.60 to 0.93. We show that in norm classification tasks {AML} is more efficient than conventional supervised machine learning approaches.},
	author = {Waltl, Bernhard and Muhr, Johannes and Glaser, Ingo and Bonczek, Georg and Scepankova, Elena and Matthes, Florian},
	date = {2017-12-13},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/FNLSRYC7/Waltl et al. - 2017 - Classifying Legal Norms with Active Machine Learni.pdf:application/pdf},
}

@article{hashmi_are_2018,
	title = {Are we done with business process compliance: state of the art and challenges ahead},
	volume = {57},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-017-1142-1},
	doi = {10.1007/s10115-017-1142-1},
	shorttitle = {Are we done with business process compliance},
	abstract = {Literature on business process compliance ({BPC}) has predominantly focused on the alignment of the regulatory rules with the design, verification and validation of business processes. Previously, surveys on {BPC} have been conducted with specific context in mind; however, the literature on {BPC} management research is largely sparse and does not accumulate a detailed understanding on existing literature and related issues faced by the domain. This survey provides a holistic view of the literature on existing {BPC} management approaches and categorises them based on different compliance management strategies in the context of formulated research questions. A systematic literature approach is used where search terms pertaining keywords were used to identify literature related to the research questions from scholarly databases. From initially 183 papers, we selected 79 papers related to the themes of this survey published between 2000 and 2015. The survey results reveal that mostly compliance management approaches centre around three distinct categories, namely design-time (\$\$28{\textbackslash}\%\$\$), run-time (\$\$32{\textbackslash}\%\$\$) and auditing (\$\$10{\textbackslash}\%\$\$). Also, organisational and internal control-based compliance management frameworks (\$\$21{\textbackslash}\%\$\$) and hybrid approaches make (\$\$9{\textbackslash}\%\$\$) of the surveyed approaches. Furthermore, open research challenges and gaps are identified and discussed with respect to the compliance problem.},
	pages = {79--133},
	number = {1},
	journaltitle = {Knowledge and Information Systems},
	shortjournal = {Knowl Inf Syst},
	author = {Hashmi, Mustafa and Governatori, Guido and Lam, Ho-Pun and Wynn, Moe Thandar},
	urldate = {2023-01-15},
	date = {2018-10-01},
	langid = {english},
	keywords = {Business process compliance, Business processes, Compliance management frameworks, Normative requirements, Norms compliance},
	file = {Eingereichte Version:/Users/jacobfehn/Zotero/storage/CR2NZ7HD/Hashmi et al. - 2018 - Are we done with business process compliance stat.pdf:application/pdf},
}

@incollection{fersini_first_2022,
	title = {A First Step Towards Automatic Consolidation of Legal Acts: Reliable Classification of Textual Modifications},
	isbn = {9791280136947},
	url = {http://books.openedition.org/aaccademia/10619},
	shorttitle = {A First Step Towards Automatic Consolidation of Legal Acts},
	abstract = {The automatic consolidation of legal texts with the integration of its successive amendments and corrigenda might have an important practical impact on public institutions, citizens and organizations. This process involves two steps: a) the classification of the textual modifications in amendment acts and b) the integration within a single document of such modifications. In this work we propose a methodology to solve step a) by exploiting Machine Learning and Natural Language Process techniques on the Italian versions of European Regulations: our results suggest that the methodology we propose is a reliable first milestone toward the automatic consolidation of legal texts.},
	pages = {141--147},
	booktitle = {Proceedings of the Eighth Italian Conference on Computational Linguistics {CliC}-it 2021},
	publisher = {Accademia University Press},
	author = {Fabrizi, Samuel and Iacono, Maria and Tesei, Andrea and De Mattei, Lorenzo},
	editor = {Fersini, Elisabetta and Passarotti, Marco and Patti, Viviana},
	urldate = {2023-01-19},
	date = {2022},
	langid = {english},
	doi = {10.4000/books.aaccademia.10619},
	file = {Fabrizi et al. - 2022 - A First Step Towards Automatic Consolidation of Le.pdf:/Users/jacobfehn/Zotero/storage/DY2HYQ6K/Fabrizi et al. - 2022 - A First Step Towards Automatic Consolidation of Le.pdf:application/pdf},
}

@article{robaldo_compiling_2012,
	title = {Compiling Regular Expressions to Extract Legal Modifications},
	url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-167-0-133},
	doi = {10.3233/978-1-61499-167-0-133},
	pages = {133--141},
	journaltitle = {Legal Knowledge and Information Systems},
	author = {Robaldo, Livio and Lesmo, Leonardo and Radicioni, Daniele P.},
	urldate = {2023-01-19},
	date = {2012},
	note = {Publisher: {IOS} Press},
}

@inproceedings{palmirani_model_2010,
	location = {Berlin, Heidelberg},
	title = {Model Regularity of Legal Language in Active Modifications},
	isbn = {978-3-642-16524-5},
	doi = {10.1007/978-3-642-16524-5_5},
	series = {Lecture Notes in Computer Science},
	abstract = {One of the main emerging challenges in legal documentation is to capture the meaning and the semantics of normative content using {NLP} techniques, and to isolate the relevant part of the linguistic speech. The last five years have seen an explosion in {XML} schemas and {DTDs} whose focus in modelling legal resources their focus was on structure. Now that the basic elements of textual descriptiveness are well formalized, we can use this knowledge to proceed with content. This paper presents a detailed methodology for classifying modificatory provisions in depth and providing all the necessary information for semi-automatically managing the consolidation process. The methodology is based on an empirical legal analysis of about 29,000 Italian acts, where we bring out regularities in the language associated with some modifications, and where we define patterns of proprieties for each type of modificatory provision. The list of verbs and the frames inferred through this empirical legal analysis have been used by the {NLP} group at the University of Turin to refine a syntactical {NLP} parser for isolating and representing the sentences as syntactic trees, and the pattern will be used by the light semantic interpreter module to indentify the parameters of modificatory provisions.},
	pages = {54--73},
	booktitle = {{AI} Approaches to the Complexity of Legal Systems. Complex Systems, the Semantic Web, Ontologies, Argumentation, and Dialogue},
	publisher = {Springer},
	author = {Palmirani, Monica and Brighi, Raffaella},
	editor = {Casanovas, Pompeu and Pagallo, Ugo and Sartor, Giovanni and Ajani, Gianmaria},
	date = {2010},
	langid = {english},
	keywords = {Legal Ontology, Legal {XML}, {NLP}},
}

@article{brighi_towards_2008,
	title = {Towards Semantic Interpretation of Legal Modifications through Deep Syntactic Analysis},
	url = {https://ebooks.iospress.nl/doi/10.3233/978-1-58603-952-3-202},
	doi = {10.3233/978-1-58603-952-3-202},
	pages = {202--206},
	journaltitle = {Legal Knowledge and Information Systems},
	author = {Brighi, Raffaella and Lesmo, Leonardo and Mazzei, Alessandro and Palmirani, Monica and Radicioni, Daniele P.},
	urldate = {2023-01-19},
	date = {2008},
	note = {Publisher: {IOS} Press},
}

@inproceedings{brighi_legal_2009,
	location = {New York, {NY}, {USA}},
	title = {Legal text analysis of the modification provisions: a pattern oriented approach},
	isbn = {978-1-60558-597-0},
	url = {https://doi.org/10.1145/1568234.1568272},
	doi = {10.1145/1568234.1568272},
	series = {{ICAIL} '09},
	shorttitle = {Legal text analysis of the modification provisions},
	abstract = {One of the main emerging research challenge in the legal documentation is to penetrate in the meaningful and in the semantic of the norm content using {NLP} techniques and isolate relevant part of the linguistic speech. This paper wants present a methodology for modeling the modificatory provisions in deep in order to provide all the necessary formalization for managing semi-automatically the consolidation process.},
	pages = {238--239},
	booktitle = {Proceedings of the 12th International Conference on Artificial Intelligence and Law},
	publisher = {Association for Computing Machinery},
	author = {Brighi, Raffaella and Palmirani, Monica},
	urldate = {2023-01-19},
	date = {2009-06-08},
	keywords = {legal {XML}, {NLP} technology, theory of law},
}

@book{schafer_legal_2012,
	title = {Legal Knowledge and Information Systems: {JURIX} 2012 : the Twenty-fifth Annual Conference},
	isbn = {978-1-61499-166-3},
	shorttitle = {Legal Knowledge and Information Systems},
	abstract = {The 25th edition of the {JURIX} conference was held in the Netherlands from the 17th till the 19th of December and was hosted by the University of Amsterdam. This year submissions came from 25 countries covering Europe, the Americas, Asia and Australia. These proceedings contain sixteen full and five short papers that were selected for presentation. As usual they cover a wide range of topics.The majority of contributions deals with formal or computational models of legal argumentation and reasoning questions of coherence, evidential reasoning, visualisation of argumentation and formal representations of legal narratives are amongst},
	pagetotal = {201},
	publisher = {{IOS} Press},
	author = {Schafer, Burkhard},
	date = {2012},
	langid = {english},
	note = {Google-Books-{ID}: 7UgMArnMIfAC},
	keywords = {Computers / Computer Science},
}

@inproceedings{biagioli_automatic_2005,
	title = {Automatic semantics extraction in law documents},
	doi = {10.1145/1165485.1165506},
	abstract = {Normative texts can be viewed as composed by formal par-titions (articles, paragraphs, etc.) or by semantic units con-taining fragments of a regulation (provisions). Provisions can be described according to a metadata scheme which con-sists of provision types and their arguments. This semantic annotation of a normative text can make the retrieval of norms easier. The detection and description of the provi-sions according to the established metadata scheme is an analytic intellectual activity aiming at classifying portions of a normative text into provision types and to extract their arguments. Automatic facilities supporting this intellectual activity are desirable. Particularly, in this paper, two mod-ules able to qualify fragments of a normative text in terms of provision types and to extract their arguments are pre-sented.},
	pages = {133--140},
	author = {Biagioli, Carlo and Francesconi, Enrico and Passerini, Andrea and Montemagni, Simonetta and Soria, Claudia},
	date = {2005-06-06},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/TPSFYG7E/Biagioli et al. - 2005 - Automatic semantics extraction in law documents.pdf:application/pdf},
}

@inproceedings{spinosa_nlp-based_2009,
	location = {New York, {NY}, {USA}},
	title = {{NLP}-based metadata extraction for legal text consolidation},
	isbn = {978-1-60558-597-0},
	url = {https://doi.org/10.1145/1568234.1568240},
	doi = {10.1145/1568234.1568240},
	series = {{ICAIL} '09},
	abstract = {The paper describes a system for the automatic consolidation of Italian legislative texts to be used as a support of an editorial consolidating activity and dealing with the following typology of textual amendments: repeal, substitution and integration. The focus of the paper is on the semantic analysis of the textual amendment provisions and the formalized representation of the amendments in terms of meta-data. The proposed approach to consolidation is metadata--oriented and based on Natural Language Processing ({NLP}) techniques: we use {XML}--based standards for metadata annotation of legislative acts and a flexible {NLP} architecture for extracting metadata from parsed texts. An evaluation of achieved results is also provided.},
	pages = {40--49},
	booktitle = {Proceedings of the 12th International Conference on Artificial Intelligence and Law},
	publisher = {Association for Computing Machinery},
	author = {Spinosa, {PierLuigi} and Giardiello, Gerardo and Cherubini, Manola and Marchi, Simone and Venturi, Giulia and Montemagni, Simonetta},
	urldate = {2023-01-19},
	date = {2009-06-08},
	keywords = {consolidation of legal text, metadata extraction, natural language processing, textual amendments, {XML} representation},
}

@inproceedings{spositto_lexical_2022,
	location = {Cham},
	title = {Lexical Analysis Using Regular Expressions for Information Retrieval from a Legal Corpus},
	isbn = {978-3-031-05903-2},
	doi = {10.1007/978-3-031-05903-2_21},
	series = {Communications in Computer and Information Science},
	abstract = {This article presents part of the work carried out in the framework of a research that aims to optimize an Information Retrieval System, by means of its specialization for the retrieval of legal documents. One of the fundamental sub-processes in this type of system is lexical analysis, in which indexing techniques are applied. These techniques involve extracting a series of concepts representative of the topics covered in a document, and then using them as access points for retrieval. This article describes a proposal for the extraction of information and identification of dates and references to named entities, such as File No., Resolution No., Article No. of Law {XXX}, which refer to the legal norm in force and are widely used in different judicial documents. For the recognition of such named entities, the process employed the definition of patterns using Regular Expressions, a way of representing a language in a synthetic form, applying a set of rules. From this, the terms obtained are stored in a matrix of terms/documents. This paper also describes the algorithms used during the validation of the proposed solution and presents the experimental results that show that by applying this method a significant reduction in the size of the inputs to the matrix can be achieved.},
	pages = {312--324},
	booktitle = {Computer Science – {CACIC} 2021},
	publisher = {Springer International Publishing},
	author = {Spositto, Osvaldo Mario and Bossero, Julio César and Moreno, Edgardo Javier and Ledesma, Viviana Alejandra and Matteo, Lorena Romina},
	editor = {Pesado, Patricia and Gil, Gustavo},
	date = {2022},
	langid = {english},
	keywords = {Information retrieval systems, Lexical analysis, Recognition of named entities, Regular expressions},
}

@inproceedings{dragoni_combining_2016,
	title = {Combining {NLP} Approaches for Rule Extraction from Legal Documents},
	url = {https://hal.science/hal-01572443},
	abstract = {Legal texts express conditions in natural language describing what is permitted, forbidden or mandatory in the context they regulate. Despite the numerous approaches tackling the problem of moving from a natural language legal text to the respective set of machine-readable conditions, results are still unsatisfiable and it remains a major open challenge. In this paper, we propose a preliminary approach which combines different Natural Language Processing techniques towards the extraction of rules from legal documents. More precisely, we combine the linguistic information provided by {WordNet} together with a syntax-based extraction of rules from legal texts, and a logic-based extraction of dependencies between chunks of such texts. Such a combined approach leads to a powerful solution towards the extraction of machine-readable rules from legal documents. We evaluate the proposed approach over the Australian " Telecommunications consumer protections code " .},
	eventtitle = {1st Workshop on {MIning} and {REasoning} with Legal texts ({MIREL} 2016)},
	author = {Dragoni, Mauro and Villata, Serena and Rizzi, Williams and Governatori, Guido},
	urldate = {2023-01-19},
	date = {2016},
	langid = {english},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/L7GWWTP9/Dragoni et al. - 2016 - Combining NLP Approaches for Rule Extraction from .pdf:application/pdf},
}

@misc{aumiller_eur-lex-sum_2022,
	title = {{EUR}-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain},
	url = {http://arxiv.org/abs/2210.13448},
	doi = {10.48550/arXiv.2210.13448},
	shorttitle = {{EUR}-Lex-Sum},
	abstract = {Existing summarization datasets come with two main drawbacks: (1) They tend to focus on overly exposed domains, such as news articles or wiki-like texts, and (2) are primarily monolingual, with few multilingual datasets. In this work, we propose a novel dataset, called {EUR}-Lex-Sum, based on manually curated document summaries of legal acts from the European Union law platform ({EUR}-Lex). Documents and their respective summaries exist as cross-lingual paragraph-aligned data in several of the 24 official European languages, enabling access to various cross-lingual and lower-resourced summarization setups. We obtain up to 1,500 document/summary pairs per language, including a subset of 375 cross-lingually aligned legal acts with texts available in all 24 languages. In this work, the data acquisition process is detailed and key characteristics of the resource are compared to existing summarization resources. In particular, we illustrate challenging sub-problems and open questions on the dataset that could help the facilitation of future research in the direction of domain-specific cross-lingual summarization. Limited by the extreme length and language diversity of samples, we further conduct experiments with suitable extractive monolingual and cross-lingual baselines for future work. Code for the extraction as well as access to our data and baselines is available online at: https://github.com/achouhan93/eur-lex-sum.},
	number = {{arXiv}:2210.13448},
	publisher = {{arXiv}},
	author = {Aumiller, Dennis and Chouhan, Ashish and Gertz, Michael},
	urldate = {2023-01-19},
	date = {2022-10-24},
	eprinttype = {arxiv},
	eprint = {2210.13448 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/jacobfehn/Zotero/storage/XX76QLYG/Aumiller et al. - 2022 - EUR-Lex-Sum A Multi- and Cross-lingual Dataset fo.pdf:application/pdf;arXiv.org Snapshot:/Users/jacobfehn/Zotero/storage/P87BB4CX/2210.html:text/html},
}

@article{vauchez_methodological_2015,
	title = {Methodological Europeanism at the Cradle: Eur-lex, the Acquis and the Making of Europe’s Cognitive Equipment},
	volume = {37},
	issn = {0703-6337},
	url = {https://doi.org/10.1080/07036337.2014.990135},
	doi = {10.1080/07036337.2014.990135},
	shorttitle = {Methodological Europeanism at the Cradle},
	abstract = {This article tracks the origins of one of Europe’s most ubiquitous instrument: the acquis. Thereby, it aims at initiating a new research program on the genealogy of Europe’s cognitive and technique equipment. Instead of considering the acquis as a self-explanatory and transparent notion, the article unearths its rich political meaning, pointing at its instrumental role in shaping a law-centered, and supranational definition of Europe. Digging deep into individual and institutional genealogies, the article follows the methodological entrepreneurs who crafted new knowledge instruments for calculating Europe’s State of affairs (the Celex database) and analyzes the process through which they have progressively acquired a monopoly in the calculation of ‘the State of the Union,’ thereby encapsulating within the very rules of the European game itself a form of ‘methodological Europeanism’.},
	pages = {193--210},
	number = {2},
	journaltitle = {Journal of European Integration},
	author = {Vauchez, Antoine},
	urldate = {2023-01-19},
	date = {2015-02-23},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/07036337.2014.990135},
	keywords = {{EU} Polity, methodological Europeanism, science studies, techniques of government},
	file = {Volltext:/Users/jacobfehn/Zotero/storage/P8VFWFGP/Vauchez - 2015 - Methodological Europeanism at the Cradle Eur-lex,.pdf:application/pdf},
}

@incollection{quaresma_using_2010,
	location = {Berlin, Heidelberg},
	title = {Using Linguistic Information and Machine Learning Techniques to Identify Entities from Juridical Documents},
	isbn = {978-3-642-12837-0},
	url = {https://doi.org/10.1007/978-3-642-12837-0_3},
	series = {Lecture Notes in Computer Science},
	abstract = {Information extraction from legal documents is an important and open problem. A mixed approach, using linguistic information and machine learning techniques, is described in this paper. In this approach, top-level legal concepts are identified and used for document classification using Support Vector Machines. Named entities, such as, locations, organizations, dates, and document references, are identified using semantic information from the output of a natural language parser. This information, legal concepts and named entities, may be used to populate a simple ontology, allowing the enrichment of documents and the creation of high-level legal information retrieval systems.},
	pages = {44--59},
	booktitle = {Semantic Processing of Legal Texts: Where the Language of Law Meets the Law of Language},
	publisher = {Springer},
	author = {Quaresma, Paulo and Gonçalves, Teresa},
	editor = {Francesconi, Enrico and Montemagni, Simonetta and Peters, Wim and Tiscornia, Daniela},
	urldate = {2023-01-19},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-3-642-12837-0_3},
	keywords = {Machine Learning, Named Entity Recognition, Natural Language Processing},
	file = {Volltext:/Users/jacobfehn/Zotero/storage/8WVL2L5A/Quaresma und Gonçalves - 2010 - Using Linguistic Information and Machine Learning .pdf:application/pdf},
}

@inproceedings{alkhatib_multi-label_2017,
	location = {Cham},
	title = {Multi-label Text Classification Using Semantic Features and Dimensionality Reduction with Autoencoders},
	isbn = {978-3-319-59888-8},
	doi = {10.1007/978-3-319-59888-8_32},
	series = {Lecture Notes in Computer Science},
	abstract = {Feature selection is of vital concern in text classification to reduce the high dimensionality of feature space. The wide range of statistical techniques which have been proposed for weighting and selecting features suffer from loss of semantic relationship among concepts and ignoring of dependencies and ordering between adjacent words. In this work we propose two techniques for incorporating semantics in feature selection. Furthermore, we use autoencoders to transform the features into a reduced feature space in order to analyse the performance penalty of feature extraction. Our intensive experiments, using the {EUR}-lex dataset, showed that semantic-based feature selection techniques significantly outperform the Bag-of-Word ({BOW}) frequency based feature selection method with term frequency/inverse document frequency ({TF}-{IDF}) for features weighting. In addition, after an aggressive dimensionality reduction of original features with a factor of 10, the autoencoders are still capable of producing better features compared to {BOW} with {TF}-{IDF}.},
	pages = {380--394},
	booktitle = {Language, Data, and Knowledge},
	publisher = {Springer International Publishing},
	author = {Alkhatib, Wael and Rensing, Christoph and Silberbauer, Johannes},
	editor = {Gracia, Jorge and Bond, Francis and {McCrae}, John P. and Buitelaar, Paul and Chiarcos, Christian and Hellmann, Sebastian},
	date = {2017},
	langid = {english},
	keywords = {Autoencoders, Dimensionality reduction, Feature selection, Semantic relations, Semantics, Text classification},
}

@inproceedings{gianfelice_modificatory_2013,
	location = {New York, {NY}, {USA}},
	title = {Modificatory provisions detection: a hybrid {NLP} approach},
	isbn = {978-1-4503-2080-1},
	url = {https://doi.org/10.1145/2514601.2514607},
	doi = {10.1145/2514601.2514607},
	series = {{ICAIL} '13},
	shorttitle = {Modificatory provisions detection},
	abstract = {In the last few years University of Turin and {CIRSFID} University of Bologna collaborated to pair {NLP} techniques and legal knowledge to detect modificatory provisions in normative texts. Annotating these modifications is a relevant and interesting problem, in that modifications affect the whole normative system; and legal language, though more regular than unrestricted language, is sometimes particularly convoluted, and poses specific linguistic issues. This paper focuses on two major aspects. First, we explore a combination between parsing and regular expressions; to the best of our knowledge, such hybrid strategy has never been proposed before to tackle the problem at hand. Secondly, we significantly extend past works coverage (basically focussed on substitution, integration and repeal modifications) in order to account for further twelve modification kinds. For the sake of conciseness, we fully illustrate and discuss only few modification types that are more relevant and interesting: suspension, prorogation of efficacy, postponement of efficacy and exception/derogation. These sorts of modifications appear particularly challenging, in that modifications in these categories make use of similar linguistic speech acts and verbs, and exhibit strong similarities in the linguistic syntactical patterns, to such an extent that to discern them is difficult for the legal expert, too. We describe the implemented system and report about an extensive experimentation on the new modificatory provisions. Results are discussed in order to improve both system's accuracy and annotation practice.},
	pages = {43--52},
	booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law},
	publisher = {Association for Computing Machinery},
	author = {Gianfelice, Davide and Lesmo, Leonardo and Palmirani, Monica and Perlo, Daniele and Radicioni, Daniele P.},
	urldate = {2023-01-21},
	date = {2013-06-10},
	keywords = {natural language processing, information extraction},
}

@article{bartolini_semantic_nodate,
	title = {Semantic mark-up of Italian legal texts through {NLP}-based techniques},
	abstract = {In this paper we illustrate an approach to information extraction from legal texts using {SALEM}. {SALEM} is an {NLP} architecture for semantic annotation and indexing of Italian legislative texts, developed by {ILC} in close collaboration with {ITTIG}-{CNR}, Florence. Results of {SALEM} performance on a test sample of about 500 Italian law paragraphs are provided.},
	author = {Bartolini, Roberto and Lenci, Alessandro and Montemagni, Simonetta and Pirrelli, Vito and Soria, Claudia},
	langid = {english},
	file = {Bartolini et al. - Semantic mark-up of Italian legal texts through NL.pdf:/Users/jacobfehn/Zotero/storage/HN8PQUNS/Bartolini et al. - Semantic mark-up of Italian legal texts through NL.pdf:application/pdf},
}

@inproceedings{garofalakis_semi-automatic_2016,
	location = {New York, {NY}, {USA}},
	title = {A semi-automatic system for the consolidation of Greek legislative texts},
	isbn = {978-1-4503-4789-1},
	url = {https://doi.org/10.1145/3003733.3003735},
	doi = {10.1145/3003733.3003735},
	series = {{PCI} '16},
	abstract = {The process of manually consolidating the historical revisions of law documents, by finding and applying in the appropriate chronological order all existing modifications to the original text, is usually tedious and mentally demanding. However, since legal language is highly structured, natural language processing techniques can be adopted to automate this process. In this paper, we present a semi-automatic system for the consolidation of Greek legislative texts, following an approach based on regular expressions. Consolidated versions of laws are pushed in a revision control system, enhancing open access to legislation. A manual step is necessary in order to fix system failures caused by syntax errors or related to wrong application of the legal rules by law makers.},
	pages = {1--6},
	booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
	publisher = {Association for Computing Machinery},
	author = {Garofalakis, John and Plessas, Konstantinos and Plessas, Athanasios},
	urldate = {2023-01-21},
	date = {2016-11-10},
	keywords = {natural language processing, Greek legislation, law revisions, law text analysis, regular expressions},
}

@inproceedings{puri_regular_2022,
	location = {Singapore},
	title = {Regular Expression-Based Text Classification Using {mSVM} and Machine Learning Techniques},
	isbn = {978-981-19331-1-0},
	doi = {10.1007/978-981-19-3311-0_17},
	series = {Algorithms for Intelligent Systems},
	abstract = {Due to the high popularity of patient electronic health record systems, effective data management is very essential nowadays. There are some significant volume of unstructured format data in the medical sector. Computer-assisted classification of such data into functional classifications such as subjects or illness may save time and money for medicine and healthcare platforms, particularly successful web healthcare providers. However, categorization in the medical sector is often difficult since even simple problems need a significant portion of domain expertise. Many strategies and techniques for medical information retrieval and categorization tasks have been established during the last few decades. The general approaches are machine learning algorithms such as Naïve Bayes, {SVM} and random forest, but still, those results have heavy noise in the generated regular expression and text. This paper proposes a collaborative natural language processing ({NLP}) and machine learning-based approach to generate practical regular expressions on extensive clinical data. The first section preprocesses data with {NLP} algorithms such as token generation, alignment generation, key generation and patter builder. This process performs the feature extraction, and those features are carried out by module training. The modified Support Vector Machine ({mSVM}) and other traditional machine learning algorithms are applied to the entire test dataset. The {SVM} produces almost 90.80–91.90\% average accuracy on test data with 15-fold cross-validation. In the result section, the overall experiment analysis has been demonstrated with various machine learning algorithms using a Weka environment.},
	pages = {199--210},
	booktitle = {Smart Data Intelligence},
	publisher = {Springer Nature},
	author = {Puri, Dinesh D. and Patnaik, G. K.},
	editor = {Asokan, R. and Ruiz, Diego P. and Baig, Zubair A. and Piramuthu, Selwyn},
	date = {2022},
	langid = {english},
	keywords = {{NLP}, Text classification, Healthcare dataset, Key extraction, Machine learning ({ML}), Regular expressions generation, Sequence alignment, Token generation},
}

@inproceedings{quemy_integrating_2020,
	location = {Cham},
	title = {On Integrating and Classifying Legal Text Documents},
	isbn = {978-3-030-59003-1},
	doi = {10.1007/978-3-030-59003-1_25},
	series = {Lecture Notes in Computer Science},
	abstract = {This paper presents an exhaustive and unified dataset based on the European Court of Human Rights judgments since its creation. The interest of such database is explained through the prism of the researcher, the data scientist, the citizen and the legal practitioner. Contrarily to many datasets, the creation process, from the collection of raw data to the feature transformation, is provided under the form of a collection of fully automated and open-source scripts. It ensures reproducibility and a high level of confidence in the processed data, which is some of the most important issues in data governance nowadays. A first experimental campaign is performed to study some predictability properties and to establish baseline results on popular machine learning algorithms. The results are consistently good across the binary datasets with an accuracy comprised between 75.86\% and 98.32\% for a micro-average accuracy of 96.44\%.},
	pages = {385--399},
	booktitle = {Database and Expert Systems Applications},
	publisher = {Springer International Publishing},
	author = {Quemy, Alexandre and Wrembel, Robert},
	editor = {Hartmann, Sven and Küng, Josef and Kotsis, Gabriele and Tjoa, A. Min and Khalil, Ismail},
	date = {2020},
	langid = {english},
	keywords = {Legal text document integration, Text analytics, Text document classification},
}

@article{korger_rule-based_nodate,
	title = {Rule-based Semantic Relation Extraction in Regulatory Documents},
	abstract = {Regulatory documents are present in many domains of daily living, technical, business, and political context. The knowledge underlying such documents is most often structured using semantic concepts from narrow to broad scope. Those concepts are immanent to the text making up a document. Narrow semantic concepts are described by some words or sentences. Semantic concepts of a broader sense are more complex in their textual representation. This work gives examples of textual characteristics of semantic concepts in the domain of nuclear safety, and that of public events. It shows a rule-based approach for the handling of these concepts and extracting the relations between them.},
	author = {Korger, Andreas and Baumeister, Joachim},
	langid = {english},
	file = {Korger und Baumeister - Rule-based Semantic Relation Extraction in Regulat.pdf:/Users/jacobfehn/Zotero/storage/IBK93C5F/Korger und Baumeister - Rule-based Semantic Relation Extraction in Regulat.pdf:application/pdf},
}

@incollection{buey_automatic_2019,
	location = {Cham},
	title = {Automatic Legal Document Analysis: Improving the Results of Information Extraction Processes Using an Ontology},
	isbn = {978-3-319-77604-0},
	url = {https://doi.org/10.1007/978-3-319-77604-0_24},
	series = {Studies in Big Data},
	shorttitle = {Automatic Legal Document Analysis},
	abstract = {Information Extraction ({IE}) is a pervasive task in the industry that allows to obtain automatically structured data from documents in natural language. Current software systems focused on this activity are able to extract a large percentage of the required information, but they do not usually focus on the quality of the extracted data. In this paper we present an approach focused on validating and improving the quality of the results of an {IE} system. Our proposal is based on the use of ontologies which store domain knowledge, and which we leverage to detect and solve consistency errors in the extracted data. We have implemented our approach to run against the output of the {AIS} system, an {IE} system specialized in analyzing legal documents and we have tested it using a real dataset. Preliminary results confirm the interest of our approach.},
	pages = {333--351},
	booktitle = {Intelligent Methods and Big Data in Industrial Applications},
	publisher = {Springer International Publishing},
	author = {Buey, María G. and Roman, Cristian and Garrido, Angel Luis and Bobed, Carlos and Mena, Eduardo},
	editor = {Bembenik, Robert and Skonieczny, Łukasz and Protaziuk, Grzegorz and Kryszkiewicz, Marzena and Rybinski, Henryk},
	urldate = {2023-01-22},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-319-77604-0_24},
	keywords = {Data curation, Information extraction, Legal document analysis, Natural language processing, Ontologies},
}

@article{lesmo_tulsi_2013,
	title = {{TULSI}: an {NLP} system for extracting legal modificatory provisions},
	volume = {21},
	doi = {10.1007/s10506-012-9127-6},
	shorttitle = {{TULSI}},
	abstract = {In this work we present the {TULSI} system (so named after Turin University Legal Semantic Interpreter), a system to produce automatic annotations of normative documents through the extraction of modificatory provisions. {TULSI} relies on a deep syntactic analysis and a shallow semantic interpreter that are illustrated in detail. We report the results of an experimental evaluation of the system and discuss them, also suggesting future directions for further improvement.},
	journaltitle = {Artificial Intelligence and Law},
	shortjournal = {Artificial Intelligence and Law},
	author = {Lesmo, Leonardo and Mazzei, Alessandro and Palmirani, Monica and Radicioni, Daniele},
	date = {2013-05-01},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/28WC4J2M/Lesmo et al. - 2013 - TULSI an NLP system for extracting legal modifica.pdf:application/pdf},
}

@article{cortes_support-vector_1995,
	title = {Support-Vector network},
	volume = {20},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1022627411411},
	doi = {10.1023/A:1022627411411},
	abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
	pages = {273--297},
	number = {3},
	journaltitle = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	urldate = {2023-01-24},
	date = {1995},
	file = {Volltext:/Users/jacobfehn/Zotero/storage/YXGKC2QL/Cortes und Vapnik - 1995 - [No title found].pdf:application/pdf},
}

@article{hevner_design_2004,
	title = {Design Science in Information Systems Research},
	volume = {28},
	abstract = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the {IS} discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and
evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the
application of these guidelines. We conclude with an analysis of
the challenges of performing high-quality design-science research in the context of the broader {IS} community.},
	pages = {75},
	journaltitle = {Management Information Systems Quarterly},
	shortjournal = {Management Information Systems Quarterly},
	author = {Hevner, Alan and R, Alan and March, Salvatore and T, Salvatore and {Park} and Park, Jinsoo and {Ram} and {Sudha}},
	date = {2004-03-01},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/VTRBYE9G/Hevner et al. - 2004 - Design Science in Information Systems Research.pdf:application/pdf},
}

@article{kitchenham_procedures_2004,
	title = {Procedures for Performing Systematic Reviews},
	volume = {33},
	journaltitle = {Keele, {UK}, Keele Univ.},
	shortjournal = {Keele, {UK}, Keele Univ.},
	author = {Kitchenham, Barbara},
	date = {2004-08-01},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/7GWPJARB/Kitchenham - 2004 - Procedures for Performing Systematic Reviews.pdf:application/pdf},
}

@article{ii_lexnlp_2021,
	title = {{LexNLP}: Natural language processing and information extraction for legal and regulatory texts},
	url = {https://www.elgaronline.com/display/edcoll/9781788972819/9781788972819.00017.xml},
	shorttitle = {{LexNLP}},
	abstract = {{LexNLP} is an open source Python package focused on natural language processing and machine learning for legal and regulatory text. The package includes functionality to (i) segment documents, (ii) identify key text such as titles and section headings, (iii) extract over eighteen types of structured information like distances and dates, (iv) extract named entities such as companies and geopolitical entities, (v) transform text into features for model training, and (vi) build unsupervised and supervised models such as word embedding or tagging models. {LexNLP} includes pre-trained models based on thousands of unit tests drawn from real documents available from the {SEC} {EDGAR} database as well as various judicial and regulatory proceedings. {LexNLP} is designed for use in both academic research and industrial applications, and is distributed at the following {GitHub} repository: https://github.com/{LexPredict}/lexpredict-lexnlp.},
	pages = {216--227},
	journaltitle = {Research Handbook on Big Data Law},
	author = {Ii, Michael J. Bommarito and Katz, Daniel Martin and Detterman, Eric M.},
	urldate = {2023-02-05},
	date = {2021-05-14},
	langid = {american},
	note = {{ISBN}: 9781788972826
Publisher: Edward Elgar Publishing
Section: Research Handbook on Big Data Law},
}

@inproceedings{zhu_learning_2008,
	location = {Columbus, Ohio},
	title = {Learning Bigrams from Unigrams},
	url = {https://aclanthology.org/P08-1075},
	eventtitle = {{ACL}-{HLT} 2008},
	pages = {656--664},
	booktitle = {Proceedings of {ACL}-08: {HLT}},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Xiaojin and Goldberg, Andrew B. and Rabbat, Michael and Nowak, Robert},
	urldate = {2023-02-05},
	date = {2008-06},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/XT2D5NRQ/Zhu et al. - 2008 - Learning Bigrams from Unigrams.pdf:application/pdf},
}

@misc{meijer_document_2021,
	title = {Document Embedding for Scientific Articles: Efficacy of Word Embeddings vs {TFIDF}},
	url = {http://arxiv.org/abs/2107.05151},
	doi = {10.48550/arXiv.2107.05151},
	shorttitle = {Document Embedding for Scientific Articles},
	abstract = {Over the last few years, neural network derived word embeddings became popular in the natural language processing literature. Studies conducted have mostly focused on the quality and application of word embeddings trained on public available corpuses such as Wikipedia or other news and social media sources. However, these studies are limited to generic text and thus lack technical and scientific nuances such as domain specific vocabulary, abbreviations, or scientific formulas which are commonly used in academic context. This research focuses on the performance of word embeddings applied to a large scale academic corpus. More specifically, we compare quality and efficiency of trained word embeddings to {TFIDF} representations in modeling content of scientific articles. We use a word2vec skip-gram model trained on titles and abstracts of about 70 million scientific articles. Furthermore, we have developed a benchmark to evaluate content models in a scientific context. The benchmark is based on a categorization task that matches articles to journals for about 1.3 million articles published in 2017. Our results show that content models based on word embeddings are better for titles (short text) while {TFIDF} works better for abstracts (longer text). However, the slight improvement of {TFIDF} for larger text comes at the expense of 3.7 times more memory requirement as well as up to 184 times higher computation times which may make it inefficient for online applications. In addition, we have created a 2-dimensional visualization of the journals modeled via embeddings to qualitatively inspect embedding model. This graph shows useful insights and can be used to find competitive journals or gaps to propose new journals.},
	number = {{arXiv}:2107.05151},
	publisher = {{arXiv}},
	author = {Meijer, H. J. and Truong, J. and Karimi, R.},
	urldate = {2023-02-05},
	date = {2021-07-11},
	eprinttype = {arxiv},
	eprint = {2107.05151 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/jacobfehn/Zotero/storage/NNXCFLLZ/Meijer et al. - 2021 - Document Embedding for Scientific Articles Effica.pdf:application/pdf;arXiv.org Snapshot:/Users/jacobfehn/Zotero/storage/7JYW3PJF/2107.html:text/html},
}

@article{waltl_semantic_2019,
	title = {Semantic types of legal norms in German laws: classification and analysis using local linear explanations},
	volume = {27},
	issn = {1572-8382},
	url = {https://doi.org/10.1007/s10506-018-9228-y},
	doi = {10.1007/s10506-018-9228-y},
	shorttitle = {Semantic types of legal norms in German laws},
	abstract = {This paper describes the automated classification of legal norms in German statutes with regard to their semantic type. We propose a semantic type taxonomy for norms in the German civil law domain consisting of nine different types focusing on functional aspects, such as Duties, Prohibitions, Permissions, etc. We performed four iterations in classifying legal norms with a rule-based approach using a manually labeled dataset, i.e., tenancy law, of the German Civil Code (\$\${\textbackslash}hbox \{n\} = 601\$\$). During this experiment the \$\$F\_1\$\$ score continuously improved from 0.52 to 0.78. In contrast, a machine learning based approach for the classification was implemented. A performance of \$\$F\_1 = 0.83\$\$ was reached. Traditionally, machine learning classifiers lack of transparency with regard to their decisions. We extended our approach using so-called local linear approximations, which is a novel technique to analyze and inspect a trained classifier’s behavior. We can show that there are significant similarities of manually crafted knowledge, i.e., rules and pattern definitions, and the trained decision structures of machine learning approaches.},
	pages = {43--71},
	number = {1},
	journaltitle = {Artificial Intelligence and Law},
	shortjournal = {Artif Intell Law},
	author = {Waltl, Bernhard and Bonczek, Georg and Scepankova, Elena and Matthes, Florian},
	urldate = {2023-02-05},
	date = {2019-03-01},
	langid = {english},
	keywords = {Natural language processing, Classifying legal norms, Explainable machine learning, Local interpretable models, Rule-based information extraction, Supervised machine learning},
}

@inproceedings{curran_linguistically_2007,
	location = {Prague, Czech Republic},
	title = {Linguistically Motivated Large-Scale {NLP} with C\&C and Boxer},
	url = {https://aclanthology.org/P07-2009},
	eventtitle = {{ACL} 2007},
	pages = {33--36},
	booktitle = {Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions},
	publisher = {Association for Computational Linguistics},
	author = {Curran, James and Clark, Stephen and Bos, Johan},
	urldate = {2023-02-06},
	date = {2007-06},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/HMPIFMBH/Curran et al. - 2007 - Linguistically Motivated Large-Scale NLP with C&C .pdf:application/pdf},
}

@online{lesmo_rule-based_2007,
	title = {The Rule-based Parser of the {NLP} Group of the University of Torino},
	url = {https://www.slideserve.com/shae/the-rule-based-parser-of-the-nlp-group-of-the-university-of-torino},
	abstract = {The Rule-based Parser of the {NLP} Group of the University of Torino. Leonardo Lesmo Dipartimento di Informatica and Centro di Scienze Cognitive, Università di Torino, Italy Email: lesmo@di.unito.it. Goals. Wide-coverage tool. Domain-independence. Extensibility to semantics. Approach.},
	titleaddon = {{SlideServe}},
	author = {Lesmo, Leonardo},
	urldate = {2023-02-06},
	date = {2007},
	langid = {english},
	file = {Snapshot:/Users/jacobfehn/Zotero/storage/2Q9MRZCK/the-rule-based-parser-of-the-nlp-group-of-the-university-of-torino.html:text/html},
}

@article{hjelseth_exploring_nodate,
	title = {{EXPLORING} {SEMANTIC} {BASED} {MODEL} {CHECKING}},
	abstract = {This paper explores the foundation of semantic based model checking concepts. Development of computable rules in a pure semantic based concept is characterized by “soft coding” by following a pre-defined mark-up methodology for linguistic (text and numbers) analysis, organization, execution and reporting. The software programming for this can be done automatically or semi-automatically based on predefined procedures. This enables a person skilled in the {AEC} domain to develop applicable rules without support of programmers. The rules can be then be applied to the semantic content of a Building Information Model, typically in the {IFC} format.},
	author = {Hjelseth, Eilif and Nisbet, Nick},
	langid = {english},
	file = {Hjelseth und Nisbet - EXPLORING SEMANTIC BASED MODEL CHECKING.pdf:/Users/jacobfehn/Zotero/storage/TW7JRF24/Hjelseth und Nisbet - EXPLORING SEMANTIC BASED MODEL CHECKING.pdf:application/pdf},
}

@article{zorzanelli_costa_capturing_2022,
	title = {On Capturing Legal Knowledge in Ontology and Process Models Combined},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA220478},
	doi = {10.3233/FAIA220478},
	pages = {267--272},
	journaltitle = {Legal Knowledge and Information Systems},
	author = {Zorzanelli Costa, Melissa and Guizzardi, Giancarlo and Almeida, Jo{\textbackslash}\&{\textbackslash}\#227 and A, o Paulo},
	urldate = {2023-02-21},
	date = {2022},
	note = {Publisher: {IOS} Press},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/Y6P9NP2M/Zorzanelli Costa et al. - 2022 - On Capturing Legal Knowledge in Ontology and Proce.pdf:application/pdf},
}

@article{ranta_end--end_2022,
	title = {An End-to-End Pipeline from Law Text to Logical Formulas},
	url = {https://ebooks.iospress.nl/doi/10.3233/FAIA220473},
	doi = {10.3233/FAIA220473},
	pages = {237--242},
	journaltitle = {Legal Knowledge and Information Systems},
	author = {Ranta, Aarne and Listenmaa, Inari and Soh, Jerrold and Wong, Meng Weng},
	urldate = {2023-02-21},
	date = {2022},
	note = {Publisher: {IOS} Press},
	file = {Full Text PDF:/Users/jacobfehn/Zotero/storage/TSEQ5956/Ranta et al. - 2022 - An End-to-End Pipeline from Law Text to Logical Fo.pdf:application/pdf},
}

@book{noauthor_11_nodate,
	title = {11 Information extraction (named entity extraction and question answering)},
	isbn = {978-1-61729-463-1},
	url = {https://learning.oreilly.com/library/view/natural-language-processing/9781617294631/OEBPS/Text/11.html},
	abstract = {11 Information extraction (named entity extraction and question answering)

  This chapter covers

  
    Sentence segmentation

    Named entity recognition ({NER})

    Numerical information...},
	urldate = {2023-10-08},
	langid = {english},
	file = {Snapshot:/Users/jacobfehn/Zotero/storage/HIL5DQIB/11.html:text/html},
}

@book{noauthor_6_nodate,
	title = {6 Reasoning with word vectors (Word2vec)},
	isbn = {978-1-61729-463-1},
	url = {https://learning.oreilly.com/library/view/natural-language-processing/9781617294631/OEBPS/Text/06.html},
	abstract = {6 Reasoning with word vectors (Word2vec)

  This chapter covers

  
    Understanding how word vectors are created

    Using pretrained models for your applications

    Reasoning with word...},
	urldate = {2023-10-08},
	langid = {english},
	file = {Snapshot:/Users/jacobfehn/Zotero/storage/733I9R8A/06.html:text/html},
}

@book{hapke_natural_2019,
	title = {Natural Language Processing in Action},
	isbn = {978-1-61729-463-1},
	url = {https://learning.oreilly.com/library/view/natural-language-processing/9781617294631/},
	abstract = {Natural Language Processing in Action is your guide to building machines that can read and interpret human language. In it, you’ll use readily available Python packages to capture the meaning in...},
	author = {Hapke, Hannes and Lane, Hobson},
	urldate = {2023-10-08},
	date = {2019-04},
	langid = {english},
	file = {Snapshot:/Users/jacobfehn/Zotero/storage/2ALLW97S/9781617294631.html:text/html},
}
